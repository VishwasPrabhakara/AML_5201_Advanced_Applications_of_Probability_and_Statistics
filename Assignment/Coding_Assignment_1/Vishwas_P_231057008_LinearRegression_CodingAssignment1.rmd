---
title: "Linear Regression Coding Assignment-1"
editor_options:
  chunk_output_type:iinline
output: html_notebook
---

```{r}
# Load essential libraries
library(ggplot2)
library(dplyr)
```


```{r}
# Load the price per square feet dataset
hData = read.csv('Data/houseprices_cleaned.csv', header = TRUE, stringsAsFactors = FALSE, na.strings = c("", "NA", "not available", "Not Available"))
str(hData)
```

```{r}
# Convert 'locality', 'facing' and 'parking' columns to factors
categorical_cols = c('locality', 'facing', 'parking')
hData[categorical_cols] = lapply(hData[categorical_cols], as.factor)
str(hData)
```

```{r}
# Continuous columns
continuous_cols = c('area', 'rent', 'price_per_sqft', 'BHK', 'bathrooms')
```

```{r}
# Plot percentage of Nans in each column of the data frame
hData_NA = setNames(stack(sapply(hData, function(x){(sum(is.na(x))/length(x))*100}))[2:1], c('Feature','Value'))
p = ggplot(data = hData_NA, aes(x = Feature, y = Value)) +
  geom_bar(stat = 'identity', fill = 'steelblue', width = 0.3) +
  theme(text = element_text(size = 14, face = 'bold'),
  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  xlab('') + ylab('Percentage') +
  ggtitle('Percentage of NAs across all features')
p
```

```{r}
# Add NA as a factor level for categorical columns 
hData[categorical_cols] = lapply(hData[categorical_cols], addNA)
str(hData)
```

```{r}
# Make a histogram of rent values
p = ggplot(data = hData) +
  geom_histogram(aes(x = rent, y = after_stat(count)), breaks = seq(mean(hData$rent)-4*sd(hData$rent), mean(hData$rent)+4*sd(hData$rent), by = 25000), color = 'black', fill = 'blue') +
  labs(x = 'Rent', y = 'Frequency')  +
   theme(axis.text = element_text(size = 8),
   axis.text.x = element_text(size = 10),
   axis.text.y = element_text(size = 10),
   axis.title = element_text(size = 10, face = "bold"))  +
  ggtitle('Histogram of house rents')
p
```

```{r}
# Build a linear model to predict price per square feet as a function of rent. How accurate is the model?
model = lm(data = hData, price_per_sqft ~ rent)
summary(model)
```

```{r}
# Make a histogram of log-transformed rent values
hData['logrent'] = log(hData['rent'])
p = ggplot(data = hData) +
  geom_histogram(aes(x = logrent, y = after_stat(count)), breaks = seq(mean(hData$logrent)-4*sd(hData$logrent), mean(hData$logrent)+4*sd(hData$logrent), by = 0.5), color = 'black', fill = 'pink') +
  labs(x = 'Rent', y = 'Frequency')  +
   theme(axis.text = element_text(size = 8),
   axis.text.x = element_text(size = 10),
   axis.text.y = element_text(size = 10),
   axis.title = element_text(size = 10, face = "bold"))  +
  ggtitle('Histogram of house rents')
p
```

```{r}
# Build a linear model to predict price per square feet as a function of logrent. Did log-transforming rent help improve the model accuracy?
model =  lm(data = hData,price_per_sqft ~ logrent)
summary(model)
```


```{r}
# Build a linear model to predict log of price per square feet as a function of logrent. Did log-transforming the response variable price per square feet improve the model accuracy?
hData['logprice_per_sqft'] = log(hData['price_per_sqft'])
model = lm(data = hData,logprice_per_sqft ~ logrent)
summary(model)
```

```{r}
# Build a linear model to predict sqrt of price per square feet as a function of logrent. Did sqrt-transforming the response variable price per square feet improve the model accuracy?
hData['sqrtprice_per_sqft'] = sqrt(hData['price_per_sqft'])
model = lm(data = hData,sqrtprice_per_sqft ~ logrent)
summary(model)
```

```{r}
# Build a linear model to predict price per sqft as a function of area and rent. Did adding area as an additional predictor improve model accuracy (compared to only rent as the predictor)? Also, interpret the coefficient estimates for area and rent practically.
model = lm(data = hData, price_per_sqft ~ area + rent )
summary(model)
```

```{r}
# Build a linear model to predict sqrt of price per sqft as a function of area and logrent. Did adding area as an additional predictor improve model accuracy (compared to only logrent as the predictor)? Also, interpret the coefficient estimates for area and logrent practically.
model = lm(data = hData,sqrtprice_per_sqft ~ area + logrent)
summary(model)
```

```{r}
# Build a linear model to predict sqrt of price per sqft as a function of logarea and logrent. Did log-transforming area improve model accuracy?
hData['logarea'] = log(hData['area'])
model = lm(data = hData,sqrtprice_per_sqft ~ logarea + logrent )
summary(model)
```

```{r}
# Build a linear model to predict price per sqft as a function of area, rent, and parking (compared to just using area and rent as predictors). Did adding parking as an additional predictor improve model accuracy?
model = lm(data = hData,price_per_sqft ~ area + rent + parking )
summary(model)
```

```{r}
# Build a linear model to predict sqrt of price per sqft as a function of logarea, logrent, and locality. Did adding locality as an additional predictor improve model accuracy (compared to just using logarea and logrent as predictors)?
model = lm(data = hData,sqrtprice_per_sqft ~ logarea + logrent + locality )
summary(model)
```

```{r}
# Build a linear model to predict price per sqft as a function of area, rent, and parking. How many levels does the categorical feature parking have? How many new variables are introduced for the categorical variable parking? Interpret all regression coefficient estimates except the intercept coefficient estimate beta0 practically. Do the p-values suggest any insignificant features (that is, features which probably don't have a linear relationship with the response variable?
model = lm(data = hData,price_per_sqft ~ area + rent + parking )
summary(model)
```

```{r}
# Create new columns corresponding to scaled versions of the continuous columns
hData[paste0('scaled_', continuous_cols)] = lapply(hData[continuous_cols], scale)
str(hData)
```

```{r}
# Build a linear model to predict scaled price per sqft as a function of scaled area and scaled rent. Compare this with the model built using unscaled data: that is, predict price per sqft as a function of area and rent. Does scaling help?
model_scaled = lm(data = hData, scaled_price_per_sqft ~ scaled_area + scaled_rent)
summary(model_scaled)
```

```{r}
# Rebuild a linear model to predict sqrt of price per sqft as a function of logarea, logrent, and locality which we will evaluate using a train-test split of the dataset
model = lm(data = hData, sqrt(price_per_sqft) ~ logarea + logrent + locality)
summary(model)
```

```{r}
# Split data into train (80%) and test (20%) sets and evaluate model performance on train and test sets. Run this cell multiple times for a random splitting of the data into train and test sets and report the model performance on the resulting train and test sets. Is there much variability in the model performance across different test sets? If that is the case, then the model is not generalizing well and is overfitting the train set. Is it the case here?
ind = sample(nrow(hData), size = floor(0.8*nrow(hData)), replace = FALSE)
hData_train = hData[ind, ]
hData_test = hData[-ind, ]

# Calculate RMSE (root-mean-squared-error) on train data
train_error = sqrt(mean((hData_train$price_per_sqft - predict(model, hData_train))^2))

# Calculate RMSE (root-mean-squared-error) on test data
test_error = sqrt(mean((hData_test$price_per_sqft - predict(model, hData_test))^2))

print(train_error)
print(test_error)
```




